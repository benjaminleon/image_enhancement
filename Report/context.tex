\chapter{Background and Context}\label{cha:cont}

In this chapter, the background and context to this thesis will be explained. After reading this chapter, the reader will be up to date with some of the recent developments in deep neural networks and other methods that have been used to enhance images. 

Some interesting techniques in neural networks regarding convergence of the networks, faster training, higher accuracy and other tricks and optimizations will be presented as well.

The text presented here is supposed to be of use to the reader to go from a knowledge-level in machine-learning and neural networks acquired from a basic university course, to a more up to date understanding of what is currently going on in the sub field which this thesis is studying. 

To start off, other fields of use for neural networks will be briefly listed. Then the background and context more directly applicable to this thesis will be explained in more details. 

\section{A two page crash course in neural networks}
Neural networks are studied for a variety of tasks. The most prominent achievement is the classification accuracy on huge data sets, the best networks are able to get a top-5 error of just a few percentages on several thousands of images the net has never seen before. When a neural net is used for classification of objects in an image, the aim is for the network to be able so "say", or indicate, what object is found in the image. For the network to be able to say that the image contains, let's say a fish, the network need to know beforehand that "fish" is an option.

If we want to build a network to be able to detect fishes, frogs, barnacles, and turtles, we provide the network with the ability to express four different "words". That is the reasoning of how we communicate with the artificial intelligence. 

However, we are not on the level of AI as in the Asimov novels, and we still understand how they are built on the very smallest of details in our computer programs. 

How it works in practice is that the network has these different "words" which it will utter by different strengths of its "voice" when it is shown a picture. The output (what the network "says") is encoded like this: [fish, frog, barnacle, turtle]. For each object, there is a separate position in a list. Neural networks speak in numbers, which we translate to words, and are thus able to understand what they say. If the network is given an image and it responds by [1, 0 0 0], then that would mean that the network is sure that there is a fish in the image, and none of the other objects. If the output is given like [0, 1, 0, 0] with the position of the frog being a 1 and the rest 0, it tells us that a frog was found in the image. 

The networks are usually trained to classify only one object in an image, and the correct answer is on the form with one 1 and the rest as zeros. This is called 1-hot encoding. 

The classification works in the way that the object is the one where the number is the largest. If the output is [0.35, 0.30, 0.15, 0.20] the network believes that the image contains a fish, since 0.35 is the largest number in the list. It is quite uncertain however, since it was a close call to "frog" whose number is only slightly smaller than the output number for "fish". 

If the correct answer in this last case indeed was "frog", the network would've made a mistake. In the large networks, there are not 4 classes (objects), but hundreds or thousands. To make the game fairer to the networks and their researchers, the networks is said to be correct if one of the five most likely objects is the correct one. This is called top-5 score, or top-5 result in the literature. 

An image is a collection of numbers, and there is one number per pixel and per color. The most commonly used color space is the RGB color space, which stands for Red, Green, and Blue. There are several other color spaces, and one which will be used later in this thesis is the LAB color space. What the network does with the image is that it performs mathematical operations such as multiplication and addition to the numbers in the image. The output of the resulting operations are fed to some linear function, then that result is modified in some additional way, and the process repeats until the final output is given. 

The different steps described above are performed in what is called "layers" of the network. The different kinds of layers are heavily experimented with as of today, and new layers and combinations of them show better and better results in some way for each paper published. 

There is always an input layer and an output layer, and the rest are called "hidden layers". The input layer is where the signal (often images, but could also be data from audio files, statistical measurements from finance or weather, and many other types of "signals"). A new method for better training and understanding of the networks is a technique called "deep visualisation", where the output from the hidden layers are exposed, and not just fed to the next layer as conventionally done. 

So how do they learn?

The numbers in the input image is multiplied with another number which is in the network, called a weight. Weight is another word for number, or numerical value in neural networks. The weights are the numbers which are changed, often ever so slightly, to make the network perform better on its task. If the network is supposed to find animals in an image and produces an output like [0.2, 0.2, 0.2, 0.2] and the correct answer is [1, 0, 0, 0], we can tune the weights to direct the output closer to the desired output, the correct answer. After the weights are changed slightly in their correct direction (which makes them larger or smaller), the produced the next time the network sees the same image [0.3, 0.1, 0.1, 0.1]. 

Note that there are usually a lot of weights in a large neural network, (millions or billions) and that they are all individually updated. A neural network with many layers is some times called a deep neural network, or DNN.  

Different kinds of loss-functions are used to be able to know how to change the weights in the network, and also to get a measurement of how well it performed on its task. One kind of loss function could be the sum of the difference between the elements in the actual output and the desired output. 

\begin{equation}
error = loss\_function(image, weights) = \sum\limits_{i} (output(i) - desired\_output(i))^2
\end{equation}


The differences are squared in the equation since there are not supposed to be any "negative" errors. Taking the absolute value would also work, but the square has another favourable property...

One can then take the derivative of the error with respect to a weight. If the derivative is positive, that means that the error will increase if the weight is increases, and if the derivative is negative the error will decrease if the weight increases. With help of this, we update all weights by increasing or decreasing them accordingly. A small step length is used, so that we carefully tune the network. If a too large step length is used, the optimum value for a weight might be overshot. Also, changing one weight will change what is a good modification of other weights in the neural network, so caution is a virtue. 

The gradients for the weights in the last layer can be calculated easiest since those weights are the closest to the analytic loss function. The derivative of the error with respect to the weights in the preceding layers is dependent on the weights in the layer closer to the end of the network. The derivative can thus be split up and calculated with help of the chain rule.  

That is the basics and some intuition about how the neural networks work. 

\section{Advances in Image Enhancement}
In this section we change gears and move into more advanced details of neural networks with their and other methods for advances in image enhancement during the recent years.

Quite a few concepts will be explained here, with references to further reading or the original papers from which these concepts or methods emerged. In the literature, the problem of improving images may be divided into two main categories: color improvement, and resolution improvement. In both categories, methods which does not include any kind of machine learning has been used previously, but seem to swiftly being phased out by learning algorithms. Since neural networks is such a new field, it is to no surprise that many researchers want to explore them. It is like a newly found mine with promise of gold and opportunities. Neural networks are showing a great deal of promise and are already  performing very well on both color improvement tasks and resolution improvement. 


\subsection{Color improvement}
In color improvement, several sub-fields are being researched, each with different views on how the image is interpreted as "better". Some recent articles have explored the task of colorizing black and white photos. This has been done with and without neural networks, some times in an end-to-end fashion and some times not. In some works, example images of similar scenes and colors to the one that shall be colorized is used in the algorithm. In some cases, a deep neural network is trained on hundreds, thousands, or a million of images to learn how to colorize black and white images. Colorization of a black and white image can also lead to higher classification accuracy performance by neural networks, as shown in the "Colorful Colorization" paper \citep{Colorful-colorization}. 

On reddit (reddit.com/r/colorization), people colorize black and white images by hand.
\Warning[TODO]{There are 2 more sub forums, add links to those too}

Apart from colorizing images which has no color, another interesting topic is to improve on image where color is already present. Such methods are of great interest to the consumer markets, image editing applications, and to companies which are working with various kinds of cameras where a subjective measure of image quality makes sense, for example video in streams. 

The last and perhaps most artistic concept in color improvement are the neural networks which are trained to morph one image so that it fits a style of another. 


\subsubsection{Colorization of black and white images}
The first paper to propose a deep neural network for automatic colorization of black and white images is named \emph{Deep Colorization} \citep{Deep-colorization}. The best results on colorization as of writing are demonstrated in the article \emph{Colorful Image Colorization} \citep{Colorful-colorization}, where several tricks are utilized to make the network predict more saturated colors than by other methods. In \emph{Automatic Colorization} \citep{Automatic-colorization}, hypercolumns are used. In \emph{Let there be Color!: Joint End-to-end Learning of Global and Local Image Priors for Automatic Image Colorization with Simultaneous Classification}, classification and colorization is performed concurrently. The image is copied into two, where each is let to run through two networks, which then merge with information about low-, mid-, and global-level features in a final colorization scheme \citep{Lettherebecolor}. An approach for colorizing black and white images without a neural network is described in \emph{Image Colorization Using Similar Images} \citep{Gupta}. When using that method, the user has to provide the algorithm with an image which has similar colors as the image to be colored. From there, the area in the black and white image will be colored from the areas in the color image where the structure is similar. Other methods collect reference images from the internet. Here a sufficiently famous object has to be present and labeled in the image, so that the system may automatically find similar reference images online and use them to colorize the black and white image. Such methods are described in \emph{Semantic Colorization with Internet Images} \citep{Semantic-Internet}, and \emph{Intrinsic Colorization} \citep{2008-intrinsic}.

\subsubsection{The averaging problem}\label{Averaging-problem}
Here the averaging problem will be discussed, and how they are solved in \emph{Colorful Colorization}, and \emph{Learning Representations for Automatic Colorization} by building probability estimates for the possible colors for each pixel. Inspired by \emph{Automatic Image Colorization Via Multimodal Predictions} \citep{Multimodal2008}, 

\subsubsection{Improving on already present colors}
There is the article \emph{Automatic Photo Adjustment Using Deep Neural Networks} \citep{Automatic-adjustment} which presents a method which is able to mimic an artist's enhancement of photos.

\subsubsection{Style transfer}
In \emph{A Neural Algorithm of Artistic Style} \citep{Gatys}, the network is trained on an image to capture the style of it and project it on another image. 
In \emph{Perceptual Losses for Real-Time Style Transfer and Super-Resolution} \citep{Perceptual-losses}, the standard pixel to pixel loss function is replaced with a loss function which is shown to work well for super-resolution. After the network has transformed an image, the resulting image is run through another neural network (called a loss network) and the output from different layers is saved and compared to the output from the same layers when the ground truth image is run through the loss network. This is where the loss function is defined, and the error gradients are backpropagated back to the trained network. In a faster way, but which yields similar results to \emph{A neural Algorithm of Artistic Style}, \emph{Texture Networks: Feed-forward Synthesis of Textures and Stylized Images} \citep{Texture-net} transform one image to fit the style of another. They also use their network to generate textures with help from samples. 

\subsection{Super resolution}
When trying to improve the resolution in an image, an up sampling is what happens. The standard ways of performing up sampling are still strong benchmarks against the newly developing techniques. The bilinear up sampling method does not produce high quality up sampling, but is good for visualising a low resolution image on a screen. The bicubic interpolation method produces blurred compared to the bilinear method, but more details become present. 

In recent papers, the developed methods still usually compare their results to the bicubic method along with other researchers' methods.

\emph{Perceptual Losses for Real-Time Style Transfer and Super-Resolution} \citep{Perceptual-losses} is not only used to perform style-transfer as described in the previous subsection, but is shown to be very succesful in upsampling images by the factors 4, and 8. Another succesful neural network approach is shown in \emph{Image Super-Resolution Using Deep Convolutional Networks} \citep{SRCNN}, where images are downsampled as a preprocessing step to training, and then upsampled again to serve as input to the network and the desired output is the original image. They let a fully convolutional neural network learn the mapping between low-resolution to high-resolution by an end to end fashion. 

\section{Tricks of the trade}
In this section, a collection of interesting methods regarding the trade of neural networks will be presented. Neural networks has been given a reputation of being black boxes, which receives inputs and gives an output; but nobody understands how and why the cogs are turning on the inside. Some of the tricks of the trade discussed in this section were developed with aims to make the training and understanding of the neural networks more transparent; they aim at opening up the black box and put a flash light to it. Others aim at speeding up the training, which can be excruciatingly slow for large models and large data sets. During training, the networks may get stuck in local optima for their loss functions; the aim of some tricks are to improve on convergence during training. Others aim at achieving higher classification accuracy. 

Despite that one trick might be developed with classification in mind, it might show beneficial to use within other sub-genres of neural networks as well, like image segmentation, super-resolution and others. 

\subsection{Deep supervision}
The technique called \emph{Deep Supervision} is presented in the paper \emph{Deeply-Supervised Nets} from 2014 \citep{Deep-supervision}. The concept is that instead of just using a loss function after the last layer in the network, extra loss functions after some of the hidden layers are added. These introduced loss functions aim at reducing the error of the so called \emph{companion objective}. The problem formulation is to find weights in layers of an artificial neural network such that a classifier at the end of it performs as well as possible. The performance from the final classifier is sought at the same time as some level of performance of the hidden layers are observed. 

One idea was that this method would tackle the diminishing gradient- and the exploding gradient problems, in which it is difficult to perform the update on the weights far away from the loss layer since the gradient of the error with respect to the weight might decrease or increase multiple times during the back propagation. 

But that is not all. Deep supervision also addresses another issue in training, namely convergence of the feature maps. Since we are peeking into the inner layers of the network, the weights can be guided towards better feature representations, faster. 

The commonly used backpropagation is still used, but in combination with the newly introduced loss functions at the hidden layers. One interesting property that the authors of the article experienced is that the testing error decreased when deep supervision was introduced, but not always the training error. The networks did converge faster, and that was especially noticeable in smaller training sets. When compared to a state-of-the-art CNN without deep supervision, both reached training error close to zero, but the deeply supervised net was able to generalize better. 

A new combined objective function is defined as follows:

\begin{equation}
||\textbf{w}^{(out)}||^2 + \mathcal{L}(W, \textbf{w}^{(out)}) + \sum\limits_{m = 1}^{M - 1}\alpha_m[|\textbf{w}^{(m)}||^2 + \ell(W, \textbf{w}^{(m)} - \gamma]_+
\end{equation}

with $\textbf{w}^{m}$ as the classifier for layer m, and M as the last layer. In the equations, the classifier is a \emph{SVM}, \emph{Support Vector Machine}, but other classifiers such as Softmax may also be used. In the original paper there was one classifier per hidden layer (and one for the output layer), but it is also possible to only supervise a few of the hidden layers in this fashion. That was done in \emph{Colorful Colorization} %/citep{Colorful-colorization}.

$\mathcal{L}(\bullet)$ is named the \emph{overall loss}, and is the classic loss function which depends only on the weights $W$ in the net producing the output, and the classification after the final layer $\textbf{w}^{(out)}$. It is defined like:

\begin{equation}
\mathcal{L}(W, \textbf{w}^{(out)}) = \sum\limits_{y_k \neq y} [1 - < \textbf{w}^{(out)}, \phi(\textbf{Z}^{(M)}, y) / \phi(\textbf{Z}^{(M)}, y_k) >]^{2}_+
\end{equation}

and

\begin{equation}\label{eq:deep_loss}
\ell(W, \textbf{w}^{(m)} = \sum\limits_{y_k \neq y} [1 - < \textbf{w}^{(m)}, \phi(\textbf{Z}^{(m)}, y) / \phi(\textbf{Z}^{(M)}, y_k) >]^{2}_+
\end{equation}

$\alpha_m$ is a weighting factor for the importance of the classification being correct in the layer $m$, and balances the importance with the output objective $\mathcal{L}$.

A contant factor $\gamma$ is included together with the $_+$ operator (0 if $[\bullet]$ is smaller than 0), because hinge loss is used. %\cite{https://en.wikipedia.org/wiki/Hinge_loss} is wikipedia ok to cite?

The $||\textbf{w}^{(m)}||$ in equation ?%\eqref{eq:deep_loss}
 is interpreted as the margin of the classifier, and we include the term to make the margin small. \Warning[TODO]{why would they want a small margin?}

$\textbf{Z}^{(m)}$ is the feature map at layer $m$ and is the result of a pooling operation on the response of the convolution in the same layer. $\textbf{Z}^{(0)}$ is thus the input.

When training the network, the gradients are updated to take the companion objective into consideration. Deep supervision works well for SGD, \emph{Stochastic Gradient Descent} and the update rules are as follows:

\begin{equation}
\frac{\partial F}{\partial \textbf{w}^{(out)}} = 2\textbf{w}^{(out)} - 2\sum\limits_{y_k \neq y}[\phi(\textbf{Z}^{(M)},y) - \phi({Z}^{(M)},y_k)][1 - <\textbf{w}^{(out)}, \phi(\textbf{Z}^{(M)},y) - \phi({Z}^{(M)},y_k>]_+
\end{equation}

\begin{equation}
\frac{\partial F}{\partial \textbf{w}^{(m)}} = 0, \text{if ||\textbf{w}$^{(m)}||^2 + \ell(W,\textbf{w}^{(m)}) \leq \gamma $}
\end{equation}

otherwise, then:

\begin{equation}
\frac{\partial F}{\partial \textbf{w}^{(m)}} =
\alpha_m\{2\textbf{w}^{(m)}-2\sum\limits_{y_k \neq y}[\phi(\textbf{Z}^{(m)},y) - \phi({Z}^{(m)},y_k)][1 - <\textbf{w}^{(m)}, \phi(\textbf{Z}^{(m)},y) - \phi({Z}^{(m)},y_k>] \}
\end{equation}

\subsection{Batch Normalization}
Since its presentation in 2015, the trick of the \emph{Batch Normalization} technique \citep{Batchnorm} has been incorporated in many architectures, with tasks including classification, colorizing black and white photos (http://tinyclouds.org/colorize/),\citep{Colorful-colorization}, style transfer \citep{Perceptual-losses} and super-resolution \citep{Perceptual-losses}. 

The trick is to make sure that the distribution of the inputs to the layers in the network are always the same. This is done by a normalization step, which is the so called Batch Normalization technique. The method accelerates the training of deep neural nets. Much higher learning rates can be used, and the need for the Dropout technique diminishes. Batch normalization also makes it possible to use saturating nonlinearities (like the hyperbolic tanh function, or the $\frac{1}{1 + e^{-x}}$ activation function) because the method helps the network from getting stuck in the saturated regions. 

\subsection{Dropout}
\emph{Dropout} \citep{Dropout} is a technique which is performed during the training phase. A random number of nodes (usually somewhere around 50\%) are ignored, meaning that their activations are 0 no matter the input, and that their weights are not updated. By not updating all the nodes at once, they are encouraged to independently find useful features and the problem with nodes being very similar to other nodes in the same layer is reduced. 

\subsection{Fully convolutional networks}
Typical networks used for classification take an input image and gives a class of the image, commonly decoded in a 1-hot vector. Say that the input to the fully connected layer are a few different NxN pixel feature maps. In the case of a fully connected final layer, the values of all pixels will be multiplied with their corresponding weights and then summed to produce the final output. 

When working with generative networks, it is instead desired to produce an image as the output. What happens in fully convolutional networks is that the fully connected layers are removed from the architecture, and replaced with convolutional layers with kernel sizes of 1x1 pixels \citep{Fully-convolutional}. In this way, all the pixels in the same spatial location in all the NxN feature maps will contribute with their respective weights to the pixels in their corresponding locations in the 32x32 output. This can be done with multiple 1x1 convolution kernels, to reduce the depth of the feature maps step by step. In the last convolutional layer, there will be only one 1x1 convolutional kernel (per output dimension, so 3 for RGB) to produce one output image. 

Upsampling is made to regain the spatial dimensions of the input image, which are usually shrunk during the convolution pipe line. 

\subsection{Hypercolumns}
Neural networks which are trained for image classification are trimmed to take notice and be discriminative on the details about structure and color, by means of feature representations, which reaches higher and higher abstraction levels deeper in the network. Usually in classification the output from the final layer and the most high-level of the feature maps are used for the classification. This seems to be sufficient and it makes sense rationally. Intuitively, it seems to be similar to how humans reason about content in images, by using only the highest level of features. 

During classification, an image is boiled down to a word, a final label of the image. In colorization however, the goal is to infer new information to the image. If sky is detected in an image, blue colors would probably be a good choice. Green goes for grass, and so on. However, more hard-to-grasp information is present in the feature maps of the lower layers as an image is propagated through a network, and can be used as additional backup for the colorization task. The features of many levels helps with clues as of what colors might be appropriate to use. 

The hypercolumn is defined at a pixel by a vector of the outputs of all nodes above that pixel. The hypercolumns for all pixels would thus be all activations at all nodes in the entire network. To use the entire hypercolumn for all pixels is extremely heavy for the GPU, and reduntant in information. Instead, sampling of the hypercolumns may be used, and all pixels does not always need to be considered, as well as all columns need not always be used. 

The term hypercolumn was coined not long ago in \citep{Hypercolumns} where the authors were able to add spatial localization to labels in images, information which is typically lost in the later layers in a DNN. Earlier layers contain better information about where something contribute to an activation than the later layers, but does not have the information about semantics. The authors were able to combine semantics and localization and named their paper \emph{Hypercolumns for Object Segmentation and Fine-grained Localization}.























